{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fff0264-3cf7-4e11-a70a-ed75bb88145b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T08:52:34.937622Z",
     "iopub.status.busy": "2022-02-25T08:52:34.937424Z",
     "iopub.status.idle": "2022-02-25T08:52:42.537735Z",
     "shell.execute_reply": "2022-02-25T08:52:42.537194Z",
     "shell.execute_reply.started": "2022-02-25T08:52:34.937599Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, GRU, Dropout, Flatten, Activation,Dropout\n",
    "from tensorflow.keras.layers import concatenate, add, Lambda\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,dropout_rate=0.1, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        self.dropout=Dropout(dropout_rate)\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        a=self.dropout(a)\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "    # def get_config\n",
    "    \n",
    "\n",
    "class LSTNet(object):\n",
    "    def __init__(self, window, dims,hidRNN,hidCNN,hidKsip,CNN_kernel, \\\n",
    "                 skip,highway_window,dropout,output_fun,,attention=True,attention_dropout=0.5):\n",
    "        super(LSTNet, self).__init__()\n",
    "        self.P = window\n",
    "        self.m = dims\n",
    "        self.hidR = hidRNN\n",
    "        self.hidC = hidCNN\n",
    "        self.hidS = hidSkip\n",
    "        self.Ck = CNN_kernel\n",
    "        self.skip = skip\n",
    "        self.pt = int((self.P-self.Ck)/self.skip)\n",
    "        self.hw = highway_window\n",
    "        self.dropout = dropout\n",
    "        self.output = output_fun\n",
    "        self.attention=attention\n",
    "        self.attention_dropout=attention_dropout\n",
    "\n",
    "    def make_model(self):\n",
    "        \n",
    "        x = Input(shape=(self.P, self.m))\n",
    "\n",
    "        # CNN\n",
    "        c = Conv1D(self.hidC, self.Ck, activation='relu')(x)\n",
    "        c = Dropout(self.dropout)(c)\n",
    "        \n",
    "\n",
    "        # skip-RNN\n",
    "        if self.attention: ## 优先使用attention\n",
    "            r = GRU(self.hidR,return_sequences=True)(c)\n",
    "            #r = Lambda(lambda k: K.reshape(k, (-1, self.hidR)))(r)\n",
    "            r = Attention(self.P,dropout_rate=self.attention_dropout)(r)\n",
    "            #r = Dropout(self.dropout)(r)\n",
    "                \n",
    "            \n",
    "        elif self.skip > 0:\n",
    "            # RNN\n",
    "            r = GRU(self.hidR)(c)\n",
    "            r = Lambda(lambda k: K.reshape(k, (-1, self.hidR)))(r)\n",
    "            r = Dropout(self.dropout)(r)\n",
    "\n",
    "            # c: batch_size*steps*filters, steps=P-Ck\n",
    "            s = Lambda(lambda k: k[:, int(-self.pt*self.skip):, :])(c)\n",
    "            s = Lambda(lambda k: K.reshape(k, (-1, self.pt, self.skip, self.hidC)))(s)\n",
    "            s = Lambda(lambda k: K.permute_dimensions(k, (0,2,1,3)))(s)\n",
    "            s = Lambda(lambda k: K.reshape(k, (-1, self.pt, self.hidC)))(s)\n",
    "\n",
    "            s = GRU(self.hidS)(s)\n",
    "            s = Lambda(lambda k: K.reshape(k, (-1, self.skip*self.hidS)))(s)\n",
    "            s = Dropout(self.dropout)(s)\n",
    "            r = concatenate([r,s])\n",
    "        \n",
    "        res = Dense(self.m)(r)\n",
    "\n",
    "        # highway\n",
    "        if self.hw > 0:\n",
    "            z = Lambda(lambda k: k[:, -self.hw:, :])(x)\n",
    "            z = Lambda(lambda k: K.permute_dimensions(k, (0,2,1)))(z)\n",
    "            z = Lambda(lambda k: K.reshape(k, (-1, self.hw)))(z)\n",
    "            z = Dense(1)(z)\n",
    "            z = Lambda(lambda k: K.reshape(k, (-1, self.m)))(z)\n",
    "            res = add([res, z])\n",
    "       \n",
    "        if self.output != 'no':\n",
    "            res = Activation(self.output)(res)\n",
    "\n",
    "        model = Model(inputs=x, outputs=res)\n",
    "        #model.compile(optimizer=Adam(lr=self.lr, clipnorm=self.clip), loss=self.loss)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e692f88-ff7e-4118-9335-cd8784a23771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
